[backbone、head、neck等深度学习中的术语解释 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/348800083)

[人工智能中图像分类、目标检测、语义分割和实例分割等任务是什么？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/109999530#:~:text=1 图像分类：判别图中物体是什么，比如是猫还是狗； 2 语义分割：对图像进行像素级分类，预测每个像素属于的类别，不区分个体；,3 目标检测：寻找图像中的物体并进行定位； 4 实例分割：定位图中每个物体，并进行像素级标注，区分不同个体；)

# Pytorch

数据集分为图像和label（txt文件）

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231020184053312.png" alt="image-20231020184053312" style="zoom:67%;" />

数据集Dataset导入

```
class MyData(Dataset):

    def __init__(self,root_dir,label_dir):
        self.root_dir = root_dir
        self.label_dir = label_dir
        self.path = os.path.join(self.root_dir,self.label_dir)
        self.img_path= os.listdir(self.path)

    def __getitem__(self,idx):
        img_name = self.img_path[idx]
        img_item_path=os.path.join(self.root_dir,self.label_dir,img_name)
        img = Image.open(img_item_path)
        label=self.label_dir
        return img,label

    def __len__(self):
        return len(self.img_path)
```

标注标签

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231020185231740.png" alt="image-20231020185231740" style="zoom:50%;" />

```
root_dir = "dataset/train/train"
target_dir = 'bees_image'
img_path = os.listdir(os.path.join(root_dir, target_dir))
label = target_dir.split('_')[0]
out_dir = 'bees_label'
for i in img_path:
    file_name = i.split('.jpg')[0]
    with open(os.path.join(root_dir, out_dir,"{}.txt".format(file_name)),'w') as f:
        f.write(label)
```



**tensorboard**

add_scalar

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231020195851658.png" alt="image-20231020195851658" style="zoom:67%;" />

```
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter("logs")
# writer.add_image()
# y=x
for i in range(100):
    writer.add_scalar("y=2x",i,i)

writer.close()
```

add_image

使用numpy型数据

```
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from PIL import Image

writer = SummaryWriter("logs")
image_path="dataset/train/train/ants_image/0013035.jpg"
img_PIL = Image.open(image_path)
img_array=np.array(img_PIL)
writer.add_image("test",img_array,1,dataformats='HWC')
```

```
def add_image(
    self, tag, img_tensor, global_step=None, walltime=None, dataformats="CHW"
):
#img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data
```

使用tensor型数据

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231020202230661.png" alt="image-20231020202230661" style="zoom:37%;" />



```
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from PIL import Image

img_path="dataset/train/train/ants_image/0013035.jpg"
img = Image.open(img_path)

writer = SummaryWriter("logs")

tensor_trans= transforms.ToTensor();
tensor_img= tensor_trans(img);

writer.add_image("TensorImage",tensor_img)
writer.close()
```

normalize：归一化

resize：改变大小

compose：将不同transforms方法合并起来使用

randomcrop：随机裁剪

```
from PIL import Image
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms

writer = SummaryWriter("logs")
img =Image.open("images/8697bca9d9655a8a2f676e36fc1a36f1.jpg")
print(img)

#Tensor
trans_totensor=transforms.ToTensor()
img_tensor=trans_totensor(img)
writer.add_image("Totensor",img_tensor)

#Normalize
trans_norm=transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])
img_norm=trans_norm(img_tensor)
writer.add_image("Normalize",img_norm)

#Resize1
print(img.size)
trans_resize=transforms.Resize((512,512))
img_resize=trans_resize(img)
img_resize=trans_totensor(img_resize)
writer.add_image("Resize",img_resize)
print(img_resize)

#Resize2(use Compose)
trans_resize_2=transforms.Resize(512)
trans_Compose=transforms.Compose([trans_resize_2,trans_totensor])
img_resize_2=trans_Compose(img)
writer.add_image("Resize2",img_resize)

writer.close()
```

testloader

```
test_loader=DataLoader(dataset=test_set,batch_size=64,shuffle=False,num_workers=0,drop_last=True)
#batch_size欸多少个数据打包成一组，shuffle为是否乱序，num_worker为使用多少个子进程进行数据加载，drop_last为是否去掉余数
```

test_loader[i]返回值为imgs,targets

模型训练

```
import torch
import torchvision
from torch import nn
from torch.nn import MaxPool2d,Conv2d,Flatten,Linear
from torch.utils.data import DataLoader

dataset_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])
dataset= torchvision.datasets.CIFAR10(root="./dataset",train=False,transform=dataset_transform,download=True)

dataloader=DataLoader(dataset=dataset,batch_size=1)
class Tom(nn.Module):
    def __init__(self):
        super(Tom, self).__init__()
        self.conv1 = Conv2d(3,32,5,padding=2)
        self.maxpool1=MaxPool2d(2)
        self.conv2 = Conv2d(32,32,5,padding=2)
        self.maxpool2 = MaxPool2d(2)
        self.conv3 = Conv2d(32, 64, 5, padding=2)
        self.maxpool3 = MaxPool2d(2)
        self.flatten = Flatten()
        self.Linear1= Linear(1024,64)
        self.Linear2= Linear(64,10)

    def forward(self,x):
        x = self.conv1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = self.maxpool2(x)
        x = self.conv3(x)
        x = self.maxpool3(x)
        x = self.flatten(x)
        x = self.Linear1(x)
        x = self.Linear2(x)
        return x

loss = nn.CrossEntropyLoss()
tom=Tom()
optim=torch.optim.SGD(tom.parameters(),lr=0.01)
for epoch in range(20):
    running_loss=0.0
    for data in dataloader:
        imgs,targets = data
        output=tom(imgs)
        loss_value=loss(output,targets)
        optim.zero_grad()
        loss_value.backward()
        optim.step()
        running_loss+=loss_value.item()
    print(running_loss)
```

l.backward()就是用来执行求解梯度的过程，而optimizer.step()则是用来执行梯度下降进行权重参数更新的过程。



保存模型/加载模型

```
torch.save(model,"vgg16_method1.pth")#保留模型结构和参数
torch.load(model,"vgg16_method1.pth")#加载
```

```
torch.save(model.state_dict(),"vgg16_method1.pth")#字典形式保留模型参数
vgg16=torchvision.model.vgg16(pretrained=False)#加载
vgg16.load_state_dict(torch.load("vgg16_method1.pth"))
```

完整项目代码

```
import torch
import torchvision
from torch import nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

from model import *
#准备数据集
train_data  = torchvision.datasets.CIFAR10(root="../data",train=True,transform=torchvision.transforms.ToTensor(),
                                           download=True)
test_data  = torchvision.datasets.CIFAR10(root="../data",train=False,transform=torchvision.transforms.ToTensor(),
                                           download=True)
#length长度
train_data_size = len(train_data)
test_data_size = len(test_data)
print("训练集的测试长度为：{}".format(train_data_size))
print("测试集的测试长度为：{}".format(test_data_size))

#利用dataloader来加载数据
train_loader = DataLoader(dataset=train_data,batch_size=64,shuffle=True)
test_loader = DataLoader(dataset=test_data,batch_size=64,shuffle=False)

#创建网络模型
tudui=Tudui()

#损失函数
loss_fn=nn.CrossEntropyLoss()

#优化器
learning_rate=1e-2
optimizer=torch.optim.SGD(tudui.parameters(),lr=learning_rate)

#记录训练的次数
total_train_step=0
#记录测试的次数
total_test_step=0
#训练的轮数
epoch=10

#添加tensorboard
writer = SummaryWriter("../logs_train")

for i in range(epoch):
    print("--------第{}轮训练开始---------".format(i+1))

    #训练步骤开始
    for data in train_loader:
        imgs,targets=data
        outputs=tudui(imgs)
        loss=loss_fn(outputs,targets)
        # 优化器优化模型
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_train_step = total_train_step+1
        if total_train_step % 100 == 0:
            print("训练次数：{}，Loss：{}".format(total_train_step,loss.item()))
            writer.add_scalar("train_loss",loss.item(),total_train_step)

    #测试步骤
    total_test_loss= 0
    total_accuracy = 0
    with torch.no_grad():
        for data in test_loader:
            imgs,targets=data
            outputs=tudui(imgs)
            loss=loss_fn(outputs,targets)
            total_test_loss=total_test_loss+loss.item()
            accuracy = (outputs.argmax(1)==targets).sum()
            total_accuracy=total_accuracy+accuracy
    print("整体数据集上的loss：{}".format(total_test_loss))
    print("整体数据集上的正确率：{}".format(total_accuracy/test_data_size))
    writer.add_scalar("test_accuracy",total_accuracy/test_data_size,total_test_step)
    writer.add_scalar("test_loss",total_test_loss,total_test_step)
    total_test_step=total_test_step+1

writer.close()
```

gpu版本

```
import torch
import torchvision
from torch import nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

# from model import *
#准备数据集
train_data  = torchvision.datasets.CIFAR10(root="../data",train=True,transform=torchvision.transforms.ToTensor(),
                                           download=True)
test_data  = torchvision.datasets.CIFAR10(root="../data",train=False,transform=torchvision.transforms.ToTensor(),
                                           download=True)
#length长度
train_data_size = len(train_data)
test_data_size = len(test_data)
print("训练集的测试长度为：{}".format(train_data_size))
print("测试集的测试长度为：{}".format(test_data_size))

#利用dataloader来加载数据
train_loader = DataLoader(dataset=train_data,batch_size=64,shuffle=True)
test_loader = DataLoader(dataset=test_data,batch_size=64,shuffle=False)

#创建网络模型
class Tudui(nn.Module):
    def __init__(self):
        super(Tudui, self).__init__()
        self.conv1 = nn.Conv2d(3,32,5,1,2)
        self.maxpool1=nn.MaxPool2d(2)
        self.conv2 = nn.Conv2d(32,32,5,1,2)
        self.maxpool2 = nn.MaxPool2d(2)
        self.conv3 = nn.Conv2d(32, 64, 5, 1,2)
        self.maxpool3 = nn.MaxPool2d(2)
        self.flatten = nn.Flatten()
        self.Linear1= nn.Linear(1024,64)
        self.Linear2= nn.Linear(64,10)

    def forward(self,x):
        x = self.conv1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = self.maxpool2(x)
        x = self.conv3(x)
        x = self.maxpool3(x)
        x = self.flatten(x)
        x = self.Linear1(x)
        x = self.Linear2(x)
        return x

if __name__ == "__main__":
    tudui=Tudui()
    input=torch.ones((64,3,32,32))
    output=tudui(input)
    print(output.shape)
tudui=Tudui()
tudui=tudui.cuda()

#损失函数
loss_fn=nn.CrossEntropyLoss()
loss_fn=loss_fn.cuda()
#优化器
learning_rate=1e-2
optimizer=torch.optim.SGD(tudui.parameters(),lr=learning_rate)

#记录训练的次数
total_train_step=0
#记录测试的次数
total_test_step=0
#训练的轮数
epoch=10

#添加tensorboard
writer = SummaryWriter("../logs_train")

for i in range(epoch):
    print("--------第{}轮训练开始---------".format(i+1))

    #训练步骤开始
    for data in train_loader:
        imgs,targets=data
        imgs=imgs.cuda()
        targets=targets.cuda()
        outputs=tudui(imgs)
        loss=loss_fn(outputs,targets)
        # 优化器优化模型
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_train_step = total_train_step+1
        if total_train_step % 100 == 0:
            print("训练次数：{}，Loss：{}".format(total_train_step,loss.item()))
            writer.add_scalar("train_loss",loss.item(),total_train_step)

    #测试步骤
    total_test_loss= 0
    total_accuracy = 0
    with torch.no_grad():
        for data in test_loader:
            imgs,targets=data
            imgs=imgs.cuda()
            targets=targets.cuda()
            outputs=tudui(imgs)
            loss=loss_fn(outputs,targets)
            total_test_loss=total_test_loss+loss.item()
            accuracy = (outputs.argmax(1)==targets).sum()
            total_accuracy=total_accuracy+accuracy
    print("整体数据集上的loss：{}".format(total_test_loss))
    print("整体数据集上的正确率：{}".format(total_accuracy/test_data_size))
    writer.add_scalar("test_accuracy",total_accuracy/test_data_size,total_test_step)
    writer.add_scalar("test_loss",total_test_loss,total_test_step)
    total_test_step=total_test_step+1

writer.close()

```

Google.colab

[深度学习基本功2：网络训练小技巧之使用预训练权重、冻结训练和断点恢复 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/446812760)

# 膨胀卷积

[膨胀卷积（Dilated convolution）-CSDN博客](https://blog.csdn.net/qq_27586341/article/details/103131674)

背景：一般做法下采样可以增大感受野，然后再下采样，但是会损失图像信息

作用：增大感受野、保持原有图像尺寸（减少信息损失）

![img](https://img-blog.csdn.net/20181007200558639?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI3NDgwMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​																					感受野为7*7，膨胀系数为2

做法：**将卷积核进行膨胀，然后再卷积**

卷积核经过膨胀后实际参与运算的卷积大小计算公式：

膨胀后的卷积核尺寸 = 膨胀系数 × (原始卷积核尺寸-1）+ 1

例如对于输入是19 x 19(暂且不考虑图像通道数)大小的图像做膨胀卷积,要使输出的图像大小保持不变，即就是仍然为19 x 19，我们要怎样实现呢?

我们的代码是基于pytorch实现的,它的卷积参数中没有padding='SAME‘的选项，padding的可取值为0,1,2,3等等的值。它的计算方式和tensorflow中的padding='VALID'的计算方式一样。

Output=(W-F+2P)/S+1

我们取strides=1,这里的原始卷积核为3 x 3大小，dilation=6,我们可以计算出膨胀后的卷积核大小为6(3-1)+1=13

带入公式可以求得：

(19-13+2*p)/1+1=19,要使这个式子成立，可以反推出padding=6。

这样一来，就可以使得输入输出的尺寸保持不变。达到了我们想要的效果

问题：**The Gridding Effect**（有空白信息的出现）

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMxLnpoaW1nLmNvbS84MC92Mi00NzhhNmI4MmUxNTA4YTE0NzcxMmFmNjNkNjQ3MmQ5YV9oZC5qcGc?x-oss-process=image/format,png)

解决：

Mi为第i层两个非0元素的距离<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231019163135740.png" alt="image-20231019163135740" style="zoom:67%;" />

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231019163142221.png" alt="image-20231019163142221" style="zoom:67%;" />

举例：kernel size K=3，an r=[1,2,5] M2=max[5-4,5-2(5-2),2]=2 M2<=K √
									      an r=[1,2,9] M2=max[9-4,9-2(9-2),2]=2 M2>K  ×

所以M1=1,才不会有空白信息出现，所以r1也要＝1，所以第一层膨胀系数需要设为1



![image-20231019165041570](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231019165041570.png)

池化步长默认为卷积核尺寸

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231022203314738.png" alt="image-20231022203314738" style="zoom:80%;" />

卷积向下取整，池化向上取整。



# MobileNet

## MobileNetV1

相比传统神经网络优点：
（1)大大减少运算量和参数数量
（2）增加超参α，β

![image-20231108095757675](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231108095757675.png)

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231108100312026.png" alt="image-20231108100312026" style="zoom:67%;" />

DW卷积的卷积核channel为1，PW卷积的卷积核大小为1

参数计算

![image-20231108100654221](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231108100654221.png)

## **MobileNetV2**

相比MobileNetV1的亮点：

（1)倒残差结构
（2)Linear bottleneck

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231108101441458.png" alt="image-20231108101441458" style="zoom:40%;" />

之前的resnet是维度中间小两边大，倒残差：先升维，后降维

原因：relu函数对低维数据损失大

结构

![image-20231108102606178](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231108102606178.png)![在这里插入图片描述](https://img-blog.csdnimg.cn/20200326132436263.png)

```
 (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
        #groups=输入输出通道意味着是dw卷积
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
```

shortcut连接条件：stride=1，输入特征矩阵和输出特征矩阵shape相同（包括channel）

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231108102707272.png" alt="image-20231108102707272" style="zoom:40%;" />

扩展因子：指的是上一张图，1×1的卷积核channel扩展倍率

shortcut：在划红线的一组，步距为1，第一次bottleneck输入shape的channel为64，输出为96，无法shortcut，第二次输入和输出的shape都为96，步距为1，可以shortcut

# FCN

全卷积网络（Fully Convolutional Networks，FCN）

**核心思想**

- 不含全连接层的全卷积网络，可适应任意尺寸输入；
- 反卷积（转制卷积）层增大图像尺寸，输出精细结果；
- 结合不同深度层结果的跳级结构，确保鲁棒性和精确性。

跳级结构
如果仅对最后一层的特征图进行上采样得到原图大小的分割，最终的分割效果往往并不理想。因为最后一层的特征图太小，这意味着过多细节的丢失。因此，通过跳级结构将最后一层的预测（富有全局信息）和更浅层（富有局部信息）的预测结合起来，在遵守全局预测的同时进行局部预测。

将底层（stride 32）的预测（FCN-32s）进行2倍的上采样得到原尺寸的图像，并与从pool4层（stride 16）进行的预测融合起来（相加），这一部分的网络被称为FCN-16s。随后将这一部分的预测再进行一次2倍的上采样并与从pool3层得到的预测融合起来，这一部分的网络被称为FCN-8s。图示如下：
<img src="https://img-blog.csdnimg.cn/img_convert/d0862cb3fb34e1caa3de6f1f5e194463.png#pic_center" alt="Skip connection" style="zoom:67%;" />

![image-20231109152355716](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231109152355716.png)![image-20231109152458099](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231109152458099.png)![image-20231109152501674](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231109152501674.png)

CNN的强大之处在于它的多层结构能自动学习特征，可学习到多个层次的特征：

- 较浅的卷积层感知域较小，学习到一些局部区域的特征；
- 较深的卷积层感知域较大，学习到更加抽象一些的特征；


抽象特征对分类很有帮助，可以很好地判断出一幅图像中包含什么类别的物体。但抽象特征对物体的大小、位置和方向等敏感性低，因为丢失了一些物体的细节，不能很好地给出物体的具体轮廓、指出每个像素具体属于哪个物体，因此难以做到精确的分割。而FCN是从抽象的特征中恢复出每个像素所属的类别，即从图像级别的分类进一步延伸到像素级别的分类。



# Deeplab



**Deeplab V1**

![在这里插入图片描述](https://img-blog.csdnimg.cn/4d51ab6f1aa24edc8c06ce7fd8d2c65e.png#pic_center)

**Deeplab V2**

<img src="https://pic1.zhimg.com/80/v2-2fd4385e4698bdf45134148fe88e1edc_1440w.webp" alt="img" style="zoom:67%;" />

其中a是原始FCN，由于下采样的存在，特征图不断降低；而b为DilatedFCN，在第block3后引入空洞卷积，在维持特征图大小的同时保证了感受野和原始网络一致。

**针对分辨率被降低的问题**:一般就是将最后的几个Maxpooling层的stride设置成1(如果是通过卷积下采样的，比如resnet，同样将stride设置成1即可)，配合使用膨胀卷积。
**针对目标多尺度的问题:**最容易想到的就是将图像缩放到多个尺度分别通过网络进行推理，最后将多个结果进行融合即可。这样做虽然有用但是计算量太大了。为了解决这个问题，DeepLab V2中提出了ASPP模块（(atrous spatial pyramid pooling) 。
**针对DCNNs不变性导致定位精度降低的问题**:和DeepLab V1差不多还是通过CRFs解决，不过这里用的是fully connected pairwise CRF，相比V1里的fully connected CRF要更高效点。



<img src="https://img-blog.csdnimg.cn/e5ae0a9d8efc4d48a4325a5620b2410b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231019174436352.png" alt="image-20231019174436352" style="zoom:67%;" />

ASPP：**针对目标多尺度的问题:**最容易想到的就是将图像缩放到多个尺度分别通过网络进行推理，最后将多个结果进行融合即可。这样做虽然有用但是计算量太大了

**Deeplab V3**

![img](https://pic2.zhimg.com/80/v2-bfa712ca4922174106c72153d9de2d2d_1440w.webp)

改进：

引入了Multi-grid(膨胀因子)
改进ASPP结构
移除CRFs后处理

训练细节：

（1）导入更大的图片

（2）上采样后才进行IOU计算

**Deeplab V3+**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191111203311239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center)

改进：相较于Deeplab V3增加了decoder-encoder结构

对于DeepLabv3，经过ASPP模块得到的特征图的output_stride为8或者16，其经过1x1的分类层后直接双线性插值到原始图片大小，这是一种非常暴力的decoder方法，特别是output_stride=16。然而这并不利于得到较精细的分割结果，故v3+模型中借鉴了EncoderDecoder结构，引入了新的Decoder模块，如下图所示。首先将encoder得到的特征双线性插值得到4x的特征，然后与encoder中对应大小的低级特征concat，如ResNet中的Conv2层，由于encoder得到的特征数只有256，而低级特征维度可能会很高，为了防止encoder得到的高级特征被弱化，先采用1x1卷积对低级特征进行降维（paper中输出维度为48）。两个特征concat后，再采用3x3卷积进一步融合特征，最后再双线性插值得到与原始图片相同大小的分割预测。（如右图）

<img src="https://pic2.zhimg.com/80/v2-fa76d61cfeeab3302a330663cc420639_1440w.webp" alt="img" style="zoom:67%;" />

# 迁移学习

迁移学习：把已训练好的模型（预训练模型）参数迁移到新的模型来帮助新模型训练
就是把别的训练模型先拿来用，当作基础。再通过自己的模型进行微调改进，以此达到更好的效果

其中，实现迁移学习有以下三种手段：

1. **Transfer Learning**：冻结预训练模型的全部[卷积层](https://www.zhihu.com/search?q=卷积层&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})，只训练自己定制的全连接层。 
2. **Extract Feature Vector**：先计算出预训练模型的卷积层对所有训练和[测试数据](https://www.zhihu.com/search?q=测试数据&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})的[特征向量](https://www.zhihu.com/search?q=特征向量&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})，然后抛开预训练模型，只训练自己定制的简配版[全连接网络](https://www.zhihu.com/search?q=全连接网络&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})。 
3. **Fine-tuning（微调）**：冻结预训练模型的部分卷积层（通常是靠近输入的多数卷积层，因为这些层保留了大量底层信息）甚至不冻结任何[网络层](https://www.zhihu.com/search?q=网络层&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})，训练剩下的卷积层（通常是靠近输出的部分卷积层）和[全连接层](https://www.zhihu.com/search?q=全连接层&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})。

Fine-tuning原理

[Fine-tuning](https://www.zhihu.com/search?q=Fine-tuning&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})的原理就是利用已知的[网络结构](https://www.zhihu.com/search?q=网络结构&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})和已知的网络参数，修改output层为我们自己的层，微调最后一层前的若干层的参数，这样就有效利用了[深度神经网络](https://www.zhihu.com/search?q=深度神经网络&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})强大的[泛化能力](https://www.zhihu.com/search?q=泛化能力&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})，又免去了设计复杂的模型以及耗时良久的训练，所以fine tuning是当数据量不足时的一个比较合适的选择。

```
# 第一步：读取当前模型参数
model_dict = model.state_dict()
# 第二步：读取预训练模型
pretrained_dict = torch.load(model_path, map_location = device)
pretrained_dict = {k: v for k, v in pretrained_dict.items() if np.shape(model_dict[k]) == np.shape(v)}
# 第三步：使用预训练的模型更新当前模型参数
model_dict.update(pretrained_dict)
# 第四步：加载模型参数
model.load_state_dict(model_dict)

#冻结训练（保存预训练模型的参数）
for param in net.features.parameters():
   param.requires_grad = False
```

np.shape(model_dict[k]) == np.shape(v) 判断了当前模型 model_dict 中键 k 对应的参数形状是否与预训练模型参数字典 pretrained_dict 中键 k 对应的参数形状相同。如果相同，则保留该键值对，否则将其过滤掉。
最终，经过这个过滤操作，得到的是一个新的预训练模型参数字典，其中只包含形状与当前模型匹配的参数。



# GoogleNet

[深入解读GoogLeNet网络结构（附代码实现）-CSDN博客](https://blog.csdn.net/qq_37555071/article/details/108214680)

网络中的亮点:
引入了Inception结构（融合不同尺度的特征信息)使用1x1的卷积核进行降维以及映射处理
添加两个辅助分类器帮助训练
丢弃全连接层，使用平均池化层(大大减少模型参数)，并采用dropout（防止过拟合）

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X2pwZy94bkJGSGNSNDE4cUM4TE9zN1R0dnJOTDRLRllpYkRIMmtpYzluQjhweXowbXFIdU0yUVk4WjZtNXNJQzlwZDRJRmJ4MlZLbWttYm1wVlg0N2lidE1VaWMwdXcvNjQw?x-oss-process=image/format,png#pic_center" alt="在这里插入图片描述" style="zoom: 50%;" />

inception结构：**利用不同大小的卷积核实现不同尺度的感知**![image-20231201115216134](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231201115216134.png)

右边添加了一个降维的功能

**一是为了避免梯度消失，用于向前传导梯度。反向传播时如果有一层求导为0，链式求导结果则为0。二是将中间某一层输出用作分类，起到模型融合作用**。

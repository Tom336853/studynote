# 1.前言

one-stage: 基于anchors直接进行分类以及调整边界框
如SSD、YOLO

two-stage: 1）通过专门模块去生成候选框(RPN)，寻找前景以及调整边界框(基于anchors)
2)基于之前生成的候选框进行进一步分类以及调整边界框(基于proposals)
如faster-rcnn



<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231201151815284.png" alt="image-20231201151815284" style="zoom:57%;" />

[IOU、GIOU、DIOU、CIOU损失函数详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/359982543)

[目标检测评价标准mAP - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/43068926)

# 2.Fast-rcnn

在深度学习时代，大名鼎鼎的RCNN和Fast RCNN依旧依赖滑窗来产生候选框，也就是Selective Search算法，该算法优化了候选框的生成策略，但仍旧会产生大量的候选框，导致即使Fast RCNN算法，在GPU上的速度也只有三、四帧每秒。直到Faster RCNN的出现，提出了RPN网络，使用RPN直接预测出候选框的位置。RPN网络一个最重要的概念就是anchor，启发了后面的SSD和YOLOv2等算法，虽然SSD算法称之为default box，也有算法叫做prior box，其实都是同一个概念，他们都是anchor的别称。

## 2.1RCNN

1.候选区域的生成

利用selective Search算法通过图像分割的方法得到一些原始区域，然后使用一些合并策略将这些区域合并，得到一个层次化的区域结构，而这些结构就包含着可能需要的物体。<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231201153738037.png" alt="image-20231201153738037" style="zoom:30%;" />

2.对每个候选区域，使用深度网络进行特征提取<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231201154006812.png" alt="image-20231201154006812" style="zoom:47%;" />

3.特征送入每一类的SVM分类器，判定类别
将2000×4096的特征矩阵与20个SVM组成的权值矩阵4096×20相乘，获得2000×20的概率矩阵，每一行代表一个建议框归于每个目标类别的概率。分别对上述2000×20维矩阵中每一列即每一类进行非极大值抑制剔除重叠建议框，得到该列即该类中得分最高的一些建议框。

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231201154527555.png" alt="image-20231201154527555" style="zoom:47%;" />

其中，左边的矩阵每一行都是一个复选框，一个复选框含有4096的特征
中间的矩阵的每一类代表一个类别的分类器，如第一列为狗的分类器，第二列为猫的分类器
右边的矩阵第一行就是第一个复选框不同类别的概率（第一行第一列为狗的概率，第一行第二列为猫的概率）

非极大值抑制剔除重叠建议框

第一步：定义 IoU 指数(Intersection over Union)，即 (A∩B) / (AUB) ，即AB的重合区域面积与AB总面积的比。直观上来讲 IoU 就是表示AB重合的比率， IoU越大说明AB的重合部分占比越大，即A和B越相似。

![img](https://img-blog.csdnimg.cn/e070d37e0fde467c8f906b83c990fcd3.png#pic_center)

第二步：找到每一类中2000个候选区域中概率最高的区域，计算其他区域与该区域的IoU值，删除所有IoU值大于阈值的候选区域。这样可以只保留少数重合率较低的候选区域，去掉重复区域。

比如下面的例子，A是向日葵类对应的所有候选框中概率最大的区域，B是另一个区域，计算AB的IoU，其结果大于阈值，那么就认为AB属于同一类（即都是向日葵），所以应该保留A，删除B，这就是非极大值抑制。
<img src="https://img-blog.csdnimg.cn/d35abfe0d0c648a288b0a54775458101.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAY3RybCBBX2N0cmwgQ19jdHJsIFY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="img" style="zoom:67%;" />

此处的删去大于给定阈值的目标是删去那些跟最高得分重叠多的部分，IOU虽大，但是分数不是最高的，分数最高的留一个就够了

4.使用回归器精修候选区域的位置

通过 Selective Search算法得到的候选区域位置不一定准确，因此用20个回归器对上述20个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后的目标区域。具体实现如下：

如图，黄色框表示候选区域 Region Proposal,绿色窗口表示实际区域Ground Truth（人工标注的），红色窗口表示 Region Proposal 进行回归后的预测区域，可以用最小二乘法解决线性回归问题。

通过回归器可以得到候选区域的四个参数，分别为：候选区域的x和y的偏移量，高度和宽度的缩放因子。可以通过这四个参数对候选区域的位置进行精修调整，就得到了红色的预测区域。
<img src="https://img-blog.csdnimg.cn/2c8d0004b8664ccf9a4c381ab6ef43a4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAY3RybCBBX2N0cmwgQ19jdHJsIFY=,size_6,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="img" style="zoom:67%;" />

## 2.2Fast RCNN

（1）一张图像生成1K~2K个候选区域(使用Selective Search算法，简称SS算法)，我们将某个候选区域称为ROI区域。

（2）将图像输入网络得到相应的特征图，将SS算法生成的候选框投影到特征图上获得相应的特征矩阵。

R-CNN vs Fast-RCNN：

R-CNN依次将2000个候选框区域输入卷积神经网络得到特征，存在大量冗余，提取时间很长。
Fast-RCNN将整张图像送入网络，一次性计算整张图像特征，这样就可以根据特征图的坐标获得想要的候选区域的特征图，不需要重复计算。

（3）将每个特征矩阵通过 ROI pooling 层缩放到7×7大小的特征图。

前面讲到RCNN需要将候选区域归一化到固定大小（227×227），而 Fast RCNN并不需要这样的操作，Fast RCNN 通过pooling层将每个候选区域的特征图都变为7×7，如下图所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/c74597c529854d588c14e16276ff8cab.png#pic_center)

## 2.3Faster RCNN

（1）将图像输入CNN网络得到相应的特征图。

（2）使用RPN网络生成候选框，将RPN生成的候选框投影到特征图上获得ROI区域的特征矩阵。

（3）将每个ROI区域的特征矩阵通过 ROI pooling 层缩放到7×7大小的特征图，接着将特征图展平为vector，之后通过一系列全连接层得到预测结果。

# 3.yolo

## 3.1yolov1

 Ground truth是真实标注框，也就是人工标注，一般被看作“真值”

Bounding box是网络最终预测的结果，也就是“可能值”，因为网络可能预测正确也可能错误

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231203152148673.png" alt="image-20231203152148673" style="zoom:37%;" />

1)将一幅图像分成SxS个网格grid cell（yolo里S-7）如果某个object的中心落在这个网格中，则这个网格就负责预测这个obje
ct。

2)每个网格要预测B个bounding box（yolo里B=2）,每个bounding box，除了要预测位置之外，还要附带预测一个confidence值（置信度在图中为框的粗细，可以简单理解为框内有物体的概率，P（Object））。每个网格还要预测c个类别的分数。

**预测阶段：**

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206153535258.png" alt="image-20231206153535258" style="zoom:67%;" />

Pascal VOC共有20类别，S=7，B=2，C=20，由输入448×448×3最终得到7×7×30的特征矩阵

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206154130467.png" alt="image-20231206154130467" style="zoom: 67%;" />

30=(2*5+20)(5表示五个参数，分别是：x,y,w,h,confidence(x,y表示预测中心点的坐标，w、h表示预测边界的宽和高）;b=2; c=20)

其中x,y,w,h都是相对值，在（0，1）之间<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231203152603672.png" alt="image-20231203152603672" style="zoom:67%;" />x,y,相对于中心网格，w,h相对于整张图像，

confidence=<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231203152810427.png" alt="image-20231203152810427" style="zoom:67%;" />网格中存在目标，则Pr（Object）=1，没有则为0

该boundingbox属于某个类别的概率=P（Object）该boundingbox的置信度×P（Cat|Object)

并且每个boundingbox最多只能预测一个类别的物体

**预测阶段后处理**

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206160430723.png" alt="image-20231206160430723" style="zoom:67%;" />

共有98个bounding box，将每个boundingbox的置信度×P（类别|Object)得到98个20×1的向量，其中的值在（0，1）之间

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206160757056.png" alt="image-20231206160757056" style="zoom:180%;" />

第一步：设置阈值（这些小于0.2的都设为0，如0.1，0.15）

第二步：将概率值排序

第三步：NMS

![image-20231206160948523](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206160948523.png)



将最大概率的框挑出来，然后其余的框跟最大的框计算iou，若iou大于设定的值，则意味着他们两个框预测的是同一个目标，则把该值设为0，保留那些iou小于设定的框

假设第二个框与第一个框iou值大于设定的值，被设为0，第三个第四个没超过，保留

继续比较第三个和第四个框，此时第三个框为最大的，第四个框若与第三个框的iou值大于设定的值则设为0

![image-20231206161324098](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206161324098.png)



![image-20231206161509226](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206161509226.png)



![image-20231206162352986](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206162352986.png)

**训练阶段**

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206162644690.png" alt="image-20231206162644690" style="zoom:30%;" />

外面的ground truth大绿框是标注的，训练目标是使ground truth所在的中心点的grid box的两个bounding box中与ground truth的iou较大的bounding box，训练拟合，与ground truth大小相近，而且该grid box的类别为狗，剩余那个bounding box和其他grid box的bounding box希望的概率值都为0

损失函数

![image-20231206163634934](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231206163634934.png)

其中，负责检测物体的bounding box的标签值为该bounding box的iou

优缺点

优点：速度快

缺点：map低，对小物体和密集物体检测准确率低，定位性能较差

## 3.2yolov2

预测主干网络：

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231210165403648.png" alt="image-20231210165403648" style="zoom:30%;" />

**Batch normalization**

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231207165558643.png" alt="image-20231207165558643" style="zoom:30%;" />

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231207165956500.png" alt="image-20231207165956500" style="zoom:37%;" />

BN层好处：加快收敛，改善梯度

**Anchor**
**Dimensior Clusters**
**Directlocation prediction**

anchors 的初始（一个矮胖，一个高瘦）是确定的，但是在训练过程中会进行微调，v1是初始随机生成两个boxes，加大了学习复杂度

与之前一样，每个grid box有5个anchor，与ground truth最大的iou的anchor来预测，而这个**预测框**只需要预测相较于自己**anchor**的偏移量（该anchor经过训练后会生成一个预测框）

共有125个参数，5个anchor，一个anchor包括4个位置，1个confidence，20个概率分布

v1只有一个bounding boxes 起到检测作用，v2所有的anchor都可以检测物体

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231207184416929.png" alt="image-20231207184416929" style="zoom:67%;" />

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231207185355160.png" alt="image-20231207185355160" style="zoom: 33%;" /><img src="https://pic4.zhimg.com/80/v2-1018c4e2739b3fd983f6a36a5093d56b_1440w.webp" alt="img" style="zoom: 80%;" />

这里是对参数进行讲解：为了防止偏移量野蛮生长，离grid box太远，限制在grid box里，使用了sigmoid函数，cx和cy为grid box左上角的坐标，所以bx和by为预测框的x坐标和y坐标。

[YOLO V2 理解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/82099160)

```text
1.对于预测的bbox的中心，需要压缩到0-1之间，再加上anchor相对于grid在x和y方向上的偏移。这一点，和yolo v1是一致的。
2.对于预测的bbox的宽高，这个和faster RCNN一样，是相对于anchor宽高的一个放缩。exp(tw)和exp(th)分别对应了宽高的放缩因子。
3.对于预测的bbox的置信度，则需要用sigmoid压缩到0-1之间。这个很合理，因为置信度就是要0-1之间。
4.对于预测的每个类别，也是用你sigmoid压缩到0-1之间。这是因为类别概率是在0-1之间。
经过以上变换之后，网络预测就都有了实际的意义。

以上得到的结果，实际上还不是最终的预测结果，以上得到的b_x，b_y，bw，bh都是在Grid这个尺度上做的，所以要乘上步长32就可以
得到在原图尺度上的预测结果。以上得到的anchor的预测结果是很多的，因此需要进行后处理，即通过非极大值抑制（NMS）和confidence
滤除不包含物体的预测，得到最终的预测结果。
```

![img](https://pic2.zhimg.com/80/v2-8f993ba170a849345332d2b60437cfb1_1440w.webp)

![image-20231210155137709](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231210155137709.png)<img src="https://pic2.zhimg.com/80/v2-90df6d8344c97e55014f877c76727299_1440w.webp" alt="img" style="zoom:67%;" />

![img](https://pic3.zhimg.com/80/v2-c1af762e76698a8a6e6558b18e42134e_1440w.webp)

```
我感觉第二行的预测框是指的模型一开始输出的五个框，第二行的作用是将这五个框尽可能的去接近五个已经设定好的Anchor（所以被称为锚框），以方便预测框后续根据真实框进行坐标回归更加快且稳定，对不对？
```

## 3.3yolov3

Backbone骨干网络：Darknet-53，darknet-53中是没有池化层的

<img src="https://pic2.zhimg.com/80/v2-e61d907d69fa6df2172d82ce7b8a6c2d_1440w.webp" alt="img" style="zoom: 33%;" />

该主干网络支持输入不同尺寸的图像，但是需要下采样32倍，所以需要是32的倍数

![image-20231212165052729](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231212165052729.png)

输入416*416的图像，下采样32倍，16倍，8倍，然后每个grid box有3个anchor，分类类别有80种，然后是xywhc，255=3×（80+5）

正负样本划分

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231210173245376.png" alt="image-20231210173244364" style="zoom:50%;" />

损失函数

其中正样本的置信度标签为1，这里采用的是logx函数，log1-log（Pc），Pc为预测值，当Pc越接近1，则损失函数越接近0，同理后面的类别损失函数。

负样本的损失函数采用的是log(1-x)，标签值为0 ，log1-log（1-x），越接近于1，损失函数越大

![image-20231210173428703](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231210173428703.png)

训练过程：每个grid box有3个bouding box（共10467个），每个bounding box 都有一个85个真实标签值，即3×85的标签值，然后拟合。![image-20231210173843509](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231210173843509.png)

v1和v2中object是pr*iou，但v3是取iou最大的，直接令object为1：与ground truth的iou最大的bounding box置信度标签值为1

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231212155950157.png" alt="image-20231212155950157" style="zoom:53%;" />

正负样本：正样本（最大的iou）对分类和定位产生贡献，而负样本（小于阈值）只对置信度产生贡献![image-20231212160801320](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231212160801320.png)

可以实现多类别分类（可以有多个种类标签值为1，预测值为1，不想之前yolov2的softmax回归，所以概率和为1）

小目标/密集目标改进
l. grid cell个数增力（兼容任意图片大小输入)

2. anchor（相当于模板，在anchor基础上生成预测框是比较简单的，比直接生成简单（yolov1））
3．多尺度预测(FPN)4．损失函数惩罚小框项
5.网络结构（骨干网络跨层连接)

## 3.4yolov4

![img](https://pic4.zhimg.com/v2-88544afd1a5b01b17f53623a0fda01db_r.jpg)

Yolov4在Yolov3的基础上进行了很多的创新。
比如**输入端**采用mosaic数据增强，
**Backbone上**采用了CSPDarknet53、Mish激活函数、Dropblock等方式，
**Neck中**采用了SPP、FPN+PAN的结构，
**输出端**则采用CIOU_Loss、DIOU_nms操作。

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231214152102699.png" alt="image-20231214152102699" style="zoom:27%;" />

预测公式改进，在原有公式中的特殊情况：掉落在边界位置，则tx和ty趋近于负无穷，这很难达到，则改进公式

[深入浅出Yolo系列之Yolov3&Yolov4&Yolov5&Yolox核心基础知识完整讲解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/143747206)

## 3.5yolov5

**backbone**

**focus结构**

<img src="https://pic3.zhimg.com/80/v2-5c6d24c95f743a31d90845a4de2e5a36_1440w.webp" alt="img" style="zoom:57%;" />

Focus结构，在Yolov3&Yolov4中并没有这个结构，其中比较关键是切片操作。

比如右图的切片示意图，4×4×3的图像切片后变成2×2×12的特征图。

以Yolov5s的结构为例，原始608×608×3的图像输入Focus结构，采用切片操作，先变成304×304×12的特征图，再经过一次32个卷积核的卷积操作，最终变成304×304×32的特征图。

**需要注意的是**：Yolov5s的Focus结构最后使用了32个卷积核，而其他三种结构，使用的数量有所增加，先注意下，后面会讲解到四种结构的不同点。

**csp结构**

<img src="https://pic1.zhimg.com/80/v2-76933f7cd2400be642a0ad48e8c401e4_1440w.webp" alt="img" style="zoom:67%;" />

**Neck**

Yolov5现在的Neck和Yolov4中一样，都采用FPN+PAN的结构，但在Yolov5刚出来时，只使用了FPN结构，后面才增加了PAN结构，此外网络中其他部分也进行了调整。

改进预测边界框高度和宽度的公式

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231214155454470.png" alt="image-20231214155454470" style="zoom:30%;" />

分配anchor

yolov3分配的是iou最大的anchor，而这里是若ground truth在anchor的0.5倍和4倍之间，则使用这个anchor

![image-20231214160121840](C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231214160121840.png)

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231214160129668.png" alt="image-20231214160129668" style="zoom:47%;" />

4.代码

train.py

<img src="C:\Users\13030\AppData\Roaming\Typora\typora-user-images\image-20231220154909271.png" alt="image-20231220154909271" style="zoom:50%;" />

[python之parser.add_argument()用法——命令行选项、参数和子命令解析器-CSDN博客](https://blog.csdn.net/qq_34243930/article/details/106517985)
